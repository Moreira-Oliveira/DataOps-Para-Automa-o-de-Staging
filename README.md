# DataOps-Para-Automacao-de-Staging

üìöDataOps Para Automa√ß√£o de Staging e Data Warehousing com Airbyte, dbt, SQL e Snowflake.

Um projeto de DataOps totalmente pr√°tico e abrangente, que integra diversas ferramentas e plataformas para implementar uma solu√ß√£o completa. O fluxo abrange desde a extra√ß√£o de dados brutos de um ambiente local at√© sua transfer√™ncia para uma √°rea de Staging na nuvem, seguida pela limpeza, transforma√ß√£o e automa√ß√£o da cria√ß√£o de um Data Warehouse na nuvem. O projeto utiliza um Modern Data Stack composto por Airbyte, dbt, SQL e a plataforma Snowflake.

![Image](https://github.com/user-attachments/assets/85958bc2-8123-4401-aa2e-52b18313aa6d)

üìö Estrat√©gias de Backup e Recupera√ß√£o para Data Warehouse na Nuvem

Garantir alta disponibilidade, integridade dos dados e tempos m√≠nimos de inatividade √© essencial para backups e recupera√ß√£o em Data Warehouses na nuvem. As principais abordagens incluem:

1. Backup Incremental e Completo

* Incremental: Armazena apenas os dados alterados ou adicionados desde o √∫ltimo backup, otimizando custos e espa√ßo de armazenamento.

* Completo: Realiza uma c√≥pia integral de todo o Data Warehouse (DW) periodicamente, garantindo uma recupera√ß√£o consistente.

2. Snapshots Automatizados

* Snapshots de Cloud Providers: Servi√ßos como AWS Redshift, Google BigQuery e Snowflake permitem configurar snapshots automatizados em intervalos regulares.

* Restaura√ß√£o F√°cil: Snapshots facilitam a recupera√ß√£o r√°pida do ambiente para um ponto espec√≠fico no tempo.

3. Estrat√©gias de Reten√ß√£o e Arquivamento

* Reten√ß√£o Personalizada: Estabele√ßa per√≠odos de reten√ß√£o diferenciados para backups di√°rios, semanais e mensais.

* Arquivamento de Dados Antigos: Mova dados pouco acessados para solu√ß√µes de armazenamento de baixo custo, como AWS S3 Glacier ou Google Coldline.

4. Replica√ß√£o Geogr√°fica

* Multirregional: Replicar o DW em v√°rias regi√µes geogr√°ficas ajuda a mitigar falhas localizadas.

* Alta Disponibilidade: Minimiza impactos causados por falhas de hardware ou interrup√ß√µes em datacenters espec√≠ficos.

5. Logs de Transa√ß√£o e Replays

* Logs de Mudan√ßa: Configure logs de transa√ß√£o ou logs de altera√ß√£o (change data capture) para rastrear modifica√ß√µes nos dados.

* Replays de Logs: Utilize os logs para restaurar dados at√© o ponto exato da falha, em caso de interrup√ß√µes.

6. Testes Regulares de Recupera√ß√£o

* Simula√ß√£o de Falhas: Realize testes peri√≥dicos para validar a efic√°cia dos processos de recupera√ß√£o.

* Redu√ß√£o de Riscos: Assegure que os backups est√£o completos, utiliz√°veis e que os tempos de recupera√ß√£o atendem aos requisitos do neg√≥cio.

7. Estrat√©gias de Versionamento

* Versionamento de Dados: Mantenha diferentes vers√µes de dados cr√≠ticos para proteger contra exclus√µes ou altera√ß√µes acidentais.

* Tabelas Temporais: Utilize recursos como "Time Travel" no Snowflake para acessar vers√µes anteriores dos dados.

8. Backups Multi-Cloud

* Redund√¢ncia Multi-Cloud: Replique backups entre diferentes provedores de nuvem para evitar depend√™ncia de um √∫nico fornecedor.

* Prote√ß√£o Adicional: Oferece seguran√ßa contra falhas generalizadas de um provedor espec√≠fico.

9. Automa√ß√£o com Ferramentas de Backup

* Ferramentas Nativas: Adote solu√ß√µes dos provedores de nuvem, como AWS Backup ou Azure Backup.

* Integra√ß√µes com Terceiros: Utilize ferramentas como Veeam, Rubrik ou Druva para automatizar e gerenciar backups em ambientes multicloud.

10. Planos de Recupera√ß√£o de Desastres (DRP)

* Procedimentos Documentados: Desenvolva e documente planos detalhados de recupera√ß√£o para diversos cen√°rios de falhas.

* Defini√ß√£o de RTO e RPO: Estabele√ßa tempos m√°ximos de recupera√ß√£o (RTO) e perdas de dados aceit√°veis (RPO) com base nos requisitos do neg√≥cio.

11. Prote√ß√£o Contra Exclus√µes e Corrup√ß√µes

* Backups Imut√°veis: Configure backups que n√£o possam ser alterados ou exclu√≠dos durante um per√≠odo definido, garantindo prote√ß√£o contra altera√ß√µes indesejadas.

* Valida√ß√£o Autom√°tica de Backups: Implemente processos automatizados para verificar regularmente a integridade e a consist√™ncia dos backups.

12. Monitoramento e Alertas

* Monitoramento Cont√≠nuo: Estabele√ßa alertas para falhas nos processos de backup ou para altera√ß√µes inesperadas nos dados.

* Respostas Proativas: Habilite interven√ß√µes r√°pidas para corrigir problemas antes que impactem opera√ß√µes cr√≠ticas.

Essas estrat√©gias devem ser personalizadas de acordo com as necessidades do neg√≥cio, o volume de dados e as regulamenta√ß√µes aplic√°veis. Uma abordagem integrada que combine automa√ß√£o, redund√¢ncia e testes regulares √© essencial para assegurar a seguran√ßa e a disponibilidade dos dados.

üìö Programa utilizado

* Airbyte
* Docker
* PostgreSQL
* Snowflake
* DBT

Para este projeto, usaremos como fonte de dados um banco de dados do PostgreSQL. Para preparar a massa de dados, vamos restaurar um banco de dados que ser√° baixado na pasta deste projeto. Ap√≥s baixar o arquivo, √© s√≥ conectar a fonte de dados no PostgreSQL.

* Resultado:

![Image](https://github.com/user-attachments/assets/42d500e6-5ac1-42b5-bf41-77721330931b)

üìö Schema do banco de dados

![Image](https://github.com/user-attachments/assets/8f3b8cbb-19a6-4781-8547-c62d66880174)

üìö Configurando a √°rea de staging no Snowflake (configura√ß√£o no arquivo txt)

A √°rea de staging em um projeto de Data Warehouse tem como objetivo atuar como uma zona intermedi√°ria para o armazenamento tempor√°rio de dados brutos provenientes de diferentes fontes. Nessa etapa, os dados s√£o carregados sem transforma√ß√µes significativas, permitindo a consolida√ß√£o, valida√ß√£o e integra√ß√£o antes de serem processados, transformados e carregados para camadas subsequentes do Data Warehouse, como a camada de integra√ß√£o ou de apresenta√ß√£o. Isso garante maior flexibilidade e controle sobre a qualidade dos dados.

![Image](https://github.com/user-attachments/assets/022ca950-6828-46e3-907e-1fcbb2526f70)

üìö Configurando EL com Airbyte

Ao configurar source e destination no Airbyte, digite as credenciais com bastante aten√ß√£o.

* Tendo instalado o Airbyte no Docker, √© s√≥ abrir diretamente por ele:

![Image](https://github.com/user-attachments/assets/c6ee6cb7-003e-4fc2-9e9a-9cfe3b07cb08)

* Ap√≥s, conecte o PostgreSQL no source e o Snowflake no destination:

![Image](https://github.com/user-attachments/assets/b008d9ab-0348-46b8-b860-d865abe4c479)

![Image](https://github.com/user-attachments/assets/a459ed7c-17b8-460a-82c8-f500e381be0c)

![Image](https://github.com/user-attachments/assets/b43d94ff-0781-4888-9f56-657629714c2c)

* Resultado no Snowflake:

![Image](https://github.com/user-attachments/assets/b7d33cf2-5c8d-4232-9e66-756e2d22b8c5)

Com isso feito iremos mandar a execu√ß√£o atravez do DBT.

A valida√ß√£o e garantia de qualidade de dados s√£o processos cr√≠ticos para assegurar que os dados utilizados em sistemas, an√°lises e decis√µes sejam precisos, consistentes, completos e confi√°veis.

Valida√ß√£o de Dados: Consiste em verificar a conformidade dos dados com regras, padr√µes e formatos predefinidos. Essa etapa identifica problemas como dados duplicados, valores ausentes, inconsist√™ncias entre fontes, erros de tipo ou viola√ß√µes de integridade referencial. A valida√ß√£o pode ocorrer em diferentes est√°gios, desde a extra√ß√£o at√© o carregamento no Data Warehouse.

Garantia de Qualidade de Dados: Envolve pr√°ticas e processos cont√≠nuos para manter e monitorar a qualidade dos dados ao longo de seu ciclo de vida. Isso inclui estabelecer m√©tricas de qualidade, automatizar auditorias, corrigir problemas identificados, rastrear a origem dos dados (linhagem de dados) e implementar governan√ßa para assegurar que os dados permane√ßam √∫teis e confi√°veis.

Esses processos s√£o fundamentais para evitar erros em an√°lises, aumentar a confian√ßa nos insights gerados e melhorar a tomada de decis√£o baseada em dados.

üìö Instalar DBT no Docker (arquivo Dockerfine no pasta)

![Image](https://github.com/user-attachments/assets/79712f07-354a-43a9-9653-0f5c8234438b)

* No container do DBT iniciamos com "dbt init projetodsa" e com isso fazer a configura√ß√£o do conector (Snoeflake)

![Image](https://github.com/user-attachments/assets/9b27951c-7413-42a7-949e-9048d9e97125)

Estando configurado, agora √© s√≥ transferir os modelos que estou fornecendo e colocar na pasta "models" que foi criada. Feito isso, volte para o Docker e execute dbt debug. Se tudo estiver configurado corretamente, o DBT estar√° conectado com o Snowflake.

![Image](https://github.com/user-attachments/assets/32b9cc0f-546f-4baf-b3f2-12f5798f4860)

* Antes de executar o dbt run, passe o arquivo YAML que estarei fornecendo para a pasta "projetodsa".

![Image](https://github.com/user-attachments/assets/a3a9e260-5c1b-43be-a012-e83fd6c20315)

üìö Modelos de Dados com DBT e SQL

S√£o representa√ß√µes de tabelas ou visualiza√ß√µes em um banco de dados, criados a partir de arquivos SQL que aplicam transforma√ß√µes nos dados. Eles permitem organizar, documentar e padronizar a l√≥gica de neg√≥cios e an√°lises em um projeto. Os modelos geralmente s√£o definidos como arquivos .sql, onde cada arquivo corresponde a uma tabela ou visualiza√ß√£o no banco de dados.

* Agora, com tudo pronto, √© s√≥ iniciar no Docker, na pasta do projeto, com dbt run, e ele ir√° iniciar os modelos:

![Image](https://github.com/user-attachments/assets/dfd15dee-f66a-40fa-aa9c-755e7f646bb4)

Com isso as tabelas dimen√ß√£o foram para o Snowflake e a tabela fato:

![Image](https://github.com/user-attachments/assets/a32b927b-9e3e-4a24-acff-07ec9a3c2fd1)

üìö Verificando a linhagem dos dados no Snowflake

√â o rastreamento da origem, movimenta√ß√£o, transforma√ß√£o e uso dos dados ao longo do tempo. Em outras palavras, √© a capacidade de entender de onde os dados v√™m, como foram processados, quem os utilizou e para quais finalidades. Essa pr√°tica √© essencial para garantir a qualidade, confiabilidade e transpar√™ncia dos dados em um sistema ou organiza√ß√£o.

![Image](https://github.com/user-attachments/assets/77964450-bdb1-4096-98a3-f07a75f5c688)

üìö Orquestra√ß√£o e Automa√ß√£o

Na  plataforma  Snowflake,  orquestra√ß√£o  e  automa√ß√£o  referem-se  ao  gerenciamento  e execu√ß√£o  de  tarefas  e  processos  de  forma  integrada  e  eficiente,  permitindo  que  pipelines  de dados sejam executados automaticamente com m√≠nima interven√ß√£o manual.

Orquestra√ß√£o:  Envolve  a  coordena√ß√£o  de  processos,  como  extra√ß√£o,  transforma√ß√£o  e carregamento (ETL/ELT), para garantir que as tarefas sejam executadas na ordem correta e de maneira  otimizada.  Snowflake  oferece  integra√ß√£o  com  ferramentas  de  orquestra√ß√£o,  como Apache Airflow e dbt, para gerenciar fluxos de trabalho complexos.

Automa√ß√£o:  Permite  configurar  processos  recorrentes  e  monitorar eventos  dentro  da plataforma.  Snowflake  oferece  funcionalidades  como  Tasks  (tarefas  agendadas  que  executam consultas  SQL  ou  scripts),  Streams  (rastreio  de  altera√ß√µes  em  dados)  e  Procedures  (blocos  de c√≥digo SQL para l√≥gica complexa), que juntos permitem criar pipelines de dados automatizados.

üìö Monitoramento e Alertas

No  Snowflake,  as  op√ß√µes  de  monitoramento  e  alertas  s√£o  projetadas  para  garantir  a visibilidade, desempenho e seguran√ßa dos ambientes de dados. As principais op√ß√µes incluem:

1. Query  History:  Ferramenta  que  permite  monitorar  o  desempenho  e  o  hist√≥rico  de execu√ß√£o de consultas, ajudando na identifica√ß√£o de gargalos e otimiza√ß√£o.

2. Resource  Monitors:  Permitem  acompanhar  e  gerenciar  o  uso  de  cr√©ditos  e  recursos computacionais. √â poss√≠vel configurar limites e criar alertas para evitar excessos de custos.

3. Account  Usage  Views:  Cole√ß√£o  de  visualiza√ß√µes  que  oferecem  dados  detalhados  sobre atividades no ambiente, como uso de armazenamento, consultas e altera√ß√µes de estrutura.

4. Integration   with   External   Tools:   Integra√ß√£o   com   ferramentas   de   monitoramento externas, como Datadog e Splunk, para alertas mais avan√ßados e centralizados.

5. Alerts with Snowflake Tasks and Procedures: Combinando tarefas agendadas, streams e procedures, √© poss√≠vel configurar alertas customizados baseados em condi√ß√µes espec√≠ficas, como falhas de processos ou anomalias de dados.

Essas  funcionalidades  ajudam  a garantir  a  opera√ß√£o  cont√≠nua  e  eficiente  do  Snowflake, al√©m de suportar pr√°ticas de governan√ßa e controle de custos

